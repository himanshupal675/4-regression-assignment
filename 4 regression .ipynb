{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa7679f-80bd-46b6-8141-6f98e97a6d24",
   "metadata": {},
   "source": [
    "## Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41c276-afbc-42ce-8a43-b8f1935c2de8",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regression is a linear regression technique that performs both regularization and feature selection. \n",
    "\n",
    "In Lasso regression, the model minimizes the sum of squared residuals, subject to a constraint that the sum of the absolute values of the coefficients is less than a fixed value. This constraint encourages the model to have sparse coefficients, effectively setting some coefficients to zero and selecting only the most relevant features for prediction. \n",
    "\n",
    "Overall, Lasso regression is a powerful regression technique that can perform feature selection and regularization simultaneously. It is particularly useful when dealing with high-dimensional data with many features and can lead to simpler, more interpretable models with better generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96840b55-0c53-40a8-8c0d-c06362e87c8c",
   "metadata": {},
   "source": [
    "# Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280de78-e39a-48c8-9e6b-e1c194b8dab5",
   "metadata": {},
   "source": [
    "The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85363e2a-4e04-4976-8164-ed18d2c451ee",
   "metadata": {},
   "source": [
    "# Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757749a-d87f-4bb8-8061-f2774e9b51ab",
   "metadata": {},
   "source": [
    "Non-Zero Coefficients: In Lasso regression, the regularization penalty can lead to some of the coefficients being shrunk to zero, resulting in a sparse model. The number of non-zero coefficients can be used to evaluate the effectiveness of the regularization and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b30e6-22c0-425f-81cd-2b0c808450ef",
   "metadata": {},
   "source": [
    "# Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188a992-206b-470d-bf9e-e82f94baf5ac",
   "metadata": {},
   "source": [
    "A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35e919-3e46-4e7f-991e-6b94e52d3303",
   "metadata": {},
   "source": [
    "# Answer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c0cda-0368-4f2f-adc1-4895fe1eac67",
   "metadata": {},
   "source": [
    "Yes, Lasso regression can be used for non-linear regression problems by introducing non-linear features or transformations of the existing features. \n",
    "\n",
    "The basic idea is to transform the original input features into a new set of features that are more suitable for linear regression. This can be achieved through various non-linear transformations, such as polynomial expansion, logarithmic transformation, or trigonometric transformation. The transformed features can then be used as input to the Lasso regression model.\n",
    "\n",
    "For example, suppose we have a non-linear regression problem where the relationship between the input variable x and the target variable y is given by the equation y = sin(x) + ε, where ε is the random error term. We can transform the input variable x into a new feature set by applying a trigonometric transformation, such as x → [sin(x), cos(x)]. The transformed features can then be used as input to a Lasso regression model, which will learn the relationship between the transformed features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c2776-2a70-4ae0-93ba-6d5b1b500e09",
   "metadata": {},
   "source": [
    "# Answer 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03de0a-0d19-4cc8-8056-cf166fa981ec",
   "metadata": {},
   "source": [
    "Ridge regression and Lasso regression are both regularization techniques used to address the problem of overfitting in linear regression models. However, they differ in the way they impose constraints on the model coefficients.\n",
    "\n",
    "The main differences between Ridge Regression and Lasso Regression are as follows:\n",
    "\n",
    "1. Penalty term: Ridge regression adds a penalty term proportional to the squared magnitude of the coefficients to the loss function, while Lasso regression adds a penalty term proportional to the absolute magnitude of the coefficients.\n",
    "\n",
    "2. Shrinkage: Ridge regression shrinks the coefficients towards zero, but does not set any coefficients to exactly zero. Lasso regression, on the other hand, can set some coefficients to exactly zero, resulting in a sparse model with fewer features.\n",
    "\n",
    "3. Feature selection: Ridge regression does not perform feature selection, meaning that it keeps all the original features in the model. Lasso regression, on the other hand, performs feature selection by setting some coefficients to zero, effectively removing the corresponding features from the model.\n",
    "\n",
    "4. Computational complexity: The optimization problem in Ridge regression involves solving a system of linear equations, which can be solved analytically using matrix algebra. The optimization problem in Lasso regression, on the other hand, involves solving a convex optimization problem, which is computationally more complex and may require numerical methods such as gradient descent or coordinate descent.\n",
    "\n",
    "5. Tuning parameter: Both Ridge regression and Lasso regression have a tuning parameter that controls the strength of the regularization penalty. However, the interpretation of the tuning parameter is different: in Ridge regression, the tuning parameter controls the amount of shrinkage, while in Lasso regression, the tuning parameter controls the sparsity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e284474-1bdf-44c9-b3ca-8eecef99a872",
   "metadata": {},
   "source": [
    "## Answer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91955d5-24f8-4573-ab0c-401485b891e9",
   "metadata": {},
   "source": [
    "Finally, LASSO regression is useful when you have some multicollinearity in your model. Multicollinearity means that the predictors variables, also known as independent variables, aren’t so independent. With multiple linear regression, this can cause your coefficients to vary dramatically and throw off the interpretability of your model. Luckily, because of LASSO’s built-in variable selection, it can handle some multicollinearity without sacrificing interpretability. If the collinearity is too high, however, LASSO’s variable selection performance will start to suffer. If there are highly correlated or collinear predictors, it will only select one of them. You’ll know if your collinearity is too high if you get a different set of predictors each time you run LASSO. If you do find that your data has a lot of multicollinearity, try using an elastic net. It’s a hybrid of ridge regression and LASSO regression that works well when multicollinearity is high. Alternatively, you can hack it by simply running LASSO multiple times, keeping track of all the significant predictors for each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b50ccf-3066-4f04-9e24-730c4c57c968",
   "metadata": {},
   "source": [
    "# Answer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f42da-1b60-4f0e-a6f8-ce59177c22e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
